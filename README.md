ğŸ¶ Whispers of the Wires

A Generative AI Music Creation Platform

ğŸ“Œ Overview

Whispers of the Wires is a full-stack generative AI web application that creates music based on user prompts or moods. The system combines state-of-the-art music generation models with audio classification techniques to deliver mood-aware, genre-informed musical compositions through an interactive web interface.

This project was developed during the AI Intern role at Infosys Springboard (Jul 2025 â€“ Nov 2025).

ğŸš€ Features

ğŸ¼ AI Music Generation using text prompts or moods

ğŸ§ Genre Classification using pretrained audio models

ğŸ˜Š Mood-aware prediction with ~85% accuracy

ğŸ“‚ Music Library & Playlists for generated tracks

ğŸŒ Full-stack web application with seamless frontendâ€“backend integration

ğŸ§  Tech Stack
Backend

FastAPI â€“ API development

Flask / HTTP Server â€“ Local model serving

PyTorch â€“ Model inference

MusicGen (Meta) â€“ Music generation

YAMNet & GTZAN â€“ Genre classification

Librosa, SciPy â€“ Audio processing

Frontend

HTML5

CSS3

JavaScript

ğŸ—ï¸ System Architecture

User provides a prompt or mood via frontend

Request is sent to FastAPI backend

MusicGen generates music audio

YAMNet + GTZAN classify genre and mood

Audio is processed using Librosa & SciPy

Generated music is returned and stored in the Library / Playlist

ğŸ“„ Pages Implemented

Home â€“ Introduction and navigation

Composition â€“ Music generation interface

Library â€“ Saved generated tracks

Playlist â€“ User-curated playlists

Predict â€“ Mood & genre prediction

âš™ï¸ Installation & Setup
Prerequisites

Python 3.9+

PyTorch

FastAPI

Flask

Librosa

SciPy

Clone the Repository
git clone https://github.com/your-username/whispers-of-the-wires.git
cd whispers-of-the-wires

Install Dependencies
pip install -r requirements.txt

Run Backend
uvicorn main:app --reload

Run Model Server
python model_server.py

Open Frontend

Open index.html in your browser or serve it via a local server.

ğŸ“Š Model Performance

Mood-aware classification accuracy: ~85%

Supports multiple genres and emotional contexts

ğŸ“š Learning Outcomes

Full-stack AI application development

Asynchronous API handling

Audio signal processing

Model deployment and integration

End-to-end coordination between frontend and backend

ğŸ”® Future Enhancements

Cloud deployment (AWS/GCP/Azure)

User authentication

Real-time audio streaming

Improved multi-label emotion detection
